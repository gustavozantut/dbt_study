{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "import pyodbc\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import udf\n",
    "from unidecode import unidecode\n",
    "from pyspark.sql.functions import lpad, regexp_replace, trim, col, concat_ws, collect_list, desc, row_number, monotonically_increasing_id \n",
    "from pyspark.sql.window import Window\n",
    "conf = SparkConf().setMaster(\"spark://192.168.0.210:7077\")\n",
    "conf.set(\"spark.executor.memory\", \"6g\")\n",
    "conf.set(\"spark.driver.memory\", \"6g\")\n",
    "conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.driver.cores\", \"4\")\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "conf.set('spark.driver.host','192.168.0.210')\n",
    "conf.set(\"spark.driver.port\", \"33139\")\n",
    "conf.set(\"spark.driver.blockManager.port\", \"45029\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"4096mb\")\n",
    "conf.set(\"spark.default.parallelism\", \"512\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "conf.set(\"spark.sql.debug.maxToStringFields\",\"200\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"128mb\")\n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1mb\")\n",
    "conf.set(\"spark.sql.autoBroadcastHashJoin\", \"12mb\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark = SparkSession.builder.config(conf=conf.setAppName(\"Jupyter\")).getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "drivers = [item for item in pyodbc.drivers()]\n",
    "driver = drivers[-1]\n",
    "connection_url = URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=\"10256589Gu@\",\n",
    "    host=\"mysql\",\n",
    "    port=1433,\n",
    "    database=\"data\",\n",
    "    query={\n",
    "        \"driver\": driver,\n",
    "        \"TrustServerCertificate\": 'yes'\n",
    "    },\n",
    ")\n",
    "engine = create_engine(\n",
    "    connection_url, connect_args={'ssl_context':True}\n",
    ")\n",
    "\n",
    "\n",
    "data_path = ('/home/jovyan/GitHub/spark notebook/data')\n",
    "\n",
    "def get_spark_df_shape(df):\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of columns: \", len(df.columns))\n",
    "    \n",
    "def save_df_to_parquet_hdfs(df, name):\n",
    "    df.columns = df.columns.map(lambda x: unidecode(x))\n",
    "    df.columns = df.columns.str.replace(' ', '', regex=False)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    save_spark_df_to_parquet_hdfs(spark_df,name)\n",
    "    \n",
    "def save_spark_df_to_parquet_hdfs(df, name):\n",
    "    df.write.mode('overwrite').partitionBy(\"spark_partition\",\"Country\").parquet(f\"hdfs://192.168.0.101:8020/data/{name}.parquet\")\n",
    "    \n",
    "def read_hdfs_parquet(filename):\n",
    "    return spark.read.parquet(f\"hdfs://192.168.0.101:8020/data/{filename}.parquet\")\n",
    "\n",
    "def spark_to_pd_head(df, n=5):\n",
    "    display(df.limit(n).toPandas().head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_df_to_parquet_hdfs(pandas.read_csv(os.path.join(data_path,\"customers-2000000.csv\")), 'customers-2000000')\n",
    "#get_spark_df_shape(df)\n",
    "#spark_to_pd_head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_hdfs_parquet(\"customers-2000000\")\n",
    "#df = df.union(df)\n",
    "#df = df.union(df)\n",
    "#df = df.union(df)\n",
    "#df = df.union(df)\n",
    "df = df.drop('Index')\n",
    "df = df.withColumn(\"spark_index\", monotonically_increasing_id())\n",
    "df = df.withColumn(\"spark_partition\", col('spark_index').substr(-2, 2).cast('int'))\n",
    "get_spark_df_shape(df)\n",
    "spark_to_pd_head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_spark_df_to_parquet_hdfs(df,'large_df')\n",
    "df =  read_hdfs_parquet(\"large_df\")\n",
    "spark_to_pd_head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(0,99):\n",
    "    \n",
    "    chunk_start_time = time.time()\n",
    "    chunk = read_hdfs_parquet(\"large_df\").filter(col('spark_index').substr(-2, 2).cast('int') == i)\n",
    "    chunk.toPandas().to_sql(name='big_table_test', con=engine, index=False, if_exists='append', schema='dbt')\n",
    "    chunk_end_time = time.time()\n",
    "    chunk_elapsed_time = chunk_end_time - chunk_start_time\n",
    "    print(f\"Progress: {(i+1)}% in {chunk_elapsed_time} seconds\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Data insertion complete in {elapsed_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT top 1000000 * FROM dbt.big_table_test', engine)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
